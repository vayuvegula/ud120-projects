{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "import tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "###features_list = ['poi','salary'] # You will need to use more features\n",
    "features_list = ['poi',\n",
    "                'salary',\n",
    "                'bonus', \n",
    "                'long_term_incentive', \n",
    "                'deferred_income', \n",
    "                'deferral_payments',\n",
    "                'loan_advances', \n",
    "                'other',\n",
    "                'expenses', \n",
    "                'director_fees',\n",
    "                'total_payments',\n",
    "                'exercised_stock_options',\n",
    "                'restricted_stock',\n",
    "                'restricted_stock_deferred',\n",
    "                'total_stock_value',\n",
    "                'to_messages',\n",
    "                'from_messages',\n",
    "                'from_this_person_to_poi',\n",
    "                'from_poi_to_this_person']\n",
    "\n",
    "#ravi code\n",
    "financial_features = [\n",
    "                'salary',\n",
    "                'bonus', \n",
    "                'long_term_incentive', \n",
    "                'deferred_income', \n",
    "                'deferral_payments',\n",
    "                'loan_advances', \n",
    "                'other',\n",
    "                'expenses', \n",
    "                'director_fees',\n",
    "                'total_payments',\n",
    "                'exercised_stock_options',\n",
    "                'restricted_stock',\n",
    "                'restricted_stock_deferred',\n",
    "                'total_stock_value']\n",
    "\n",
    "email_features = [  'to_messages',\n",
    "                'from_messages',\n",
    "                'from_this_person_to_poi',\n",
    "                'from_poi_to_this_person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform data from dictionary to the Pandas DataFrame\n",
    "df = pd.DataFrame.from_dict(data_dict, orient = 'index')\n",
    "#Order columns in DataFrame, exclude email column\n",
    "df = df[features_list].astype('float64')\n",
    "df[financial_features]=df[financial_features].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Ravi Code\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "for train_index, test_index in split.split(df,df['poi']):\n",
    "    strat_train_set = df.iloc[train_index]\n",
    "    strat_test_set = df.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Ravi Code\n",
    "##creating a custom transformer for column attributes\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "import pandas as pd\n",
    "class CombinedAttributersAdder(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,fraction_from_poi=False):\n",
    "        \n",
    "        self.fraction_from_poi = fraction_from_poi\n",
    "    def fit(self,X,y=None):\n",
    "        return self #nothing else to do\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X['fraction_to_poi']=X['from_this_person_to_poi'] / X['to_messages']\n",
    "        X['fraction_from_poi']=X['from_poi_to_this_person'] / X['from_messages']\n",
    "        X['fraction_of_overall_to_poi']=X['from_poi_to_this_person'] / X['from_poi_to_this_person'].sum()\n",
    "        X['fraction_of_overall_from_poi']=X['from_poi_to_this_person'] / X['from_poi_to_this_person'].sum()\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ravi code\n",
    "#creating the pipeline and running it\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#num_pipeline = Pipeline([\n",
    "#    ('imputer',Imputer(strategy='median')),\n",
    "#    ('attribs_adder',CombinedAttributersAdder()),\n",
    "#    ('std_scaler',StandardScaler()),\n",
    "#])\n",
    "\n",
    "#train_data_pipeline = num_pipeline.fit_transform(strat_train_set)\n",
    "#train_data_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=strat_train_set.drop('TOTAL')\n",
    "\n",
    "#df.iloc[:,:] = df.iloc[:,:].fillna(0)\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "\n",
    "#impute missing values of email features \n",
    "df.loc[df[df.poi == 1].index,email_features] = imp.fit_transform(df[email_features][df.poi == 1])\n",
    "df.loc[df[df.poi == 0].index,email_features] = imp.fit_transform(df[email_features][df.poi == 0])\n",
    "\n",
    "attr=CombinedAttributersAdder()\n",
    "df_new = attr.transform(df)\n",
    "#df_new.to_csv(\"../df_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df_norm = scaler.fit_transform(df_new.drop('poi',axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features=df_norm\n",
    "target = df_new['poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#modifying the testdata\n",
    "#impute the missing values\n",
    "dftest=strat_test_set\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "\n",
    "#impute missing values of email features\n",
    "dftest.loc[dftest[dftest.poi == 1].index,email_features] = imp.fit_transform(dftest[email_features][dftest.poi == 1])\n",
    "dftest.loc[dftest[dftest.poi == 0].index,email_features] = imp.fit_transform(dftest[email_features][dftest.poi == 0])\n",
    "#attr2=CombinedAttributersAdder()\n",
    "df_new_test = attr.transform(dftest)\n",
    "#df_new.to_csv(\"../df_new.csv\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_norm_test = scaler.fit_transform(df_new_test.drop('poi',axis=1))\n",
    "\n",
    "features_test=df_norm_test\n",
    "target_test=df_new_test['poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision tree using features with non-null importance\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score,precision_recall_curve\n",
    "clf = DecisionTreeClassifier(random_state = 75)\n",
    "clf=clf.fit(features,target)\n",
    "#target_score=cross_val_score(clf,features,target,cv=3,)\n",
    "target_pred = cross_val_predict(clf,features,target,cv=3)\n",
    "confusion_matrix = confusion_matrix(target,target_pred)\n",
    "precision_score(target,target_pred)\n",
    "recall_score(target,target_pred)\n",
    "##does not seem to be the best algo to use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exercised_stock_options', 0.25997425997425994]\n",
      "['from_this_person_to_poi', 0.23139324412308845]\n",
      "['other', 0.19152935701733442]\n",
      "['to_messages', 0.12941953536012948]\n",
      "['total_payments', 0.11374763850011378]\n",
      "['fraction_of_overall_from_poi', 0.040664780763790674]\n",
      "['expenses', 0.033271184261283289]\n"
     ]
    }
   ],
   "source": [
    "# show the features with non null importance, sorted and create features_list of features for the model\n",
    "features_importance = []\n",
    "for i in range(len(clf.feature_importances_)):\n",
    "    if clf.feature_importances_[i] > 0:\n",
    "        features_importance.append([df.columns[i+1], clf.feature_importances_[i]])\n",
    "features_importance.sort(key=lambda x: x[1], reverse = True)\n",
    "for f_i in features_importance:\n",
    "    print f_i\n",
    "features_list = [x[0] for x in features_importance]\n",
    "features_list.insert(0, 'poi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.87      1.00      0.93        26\n",
      "        1.0       0.00      0.00      0.00         4\n",
      "\n",
      "avg / total       0.75      0.87      0.80        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vayuvegula/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Searchgrid for random forest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "# specify parameters and distributions to sample from\n",
    "param_grid = {'bootstrap': [False],\n",
    " 'criterion': ['entropy'],\n",
    " 'max_depth': [None],\n",
    " 'max_features': [1],\n",
    " 'min_samples_leaf': [1],\n",
    " 'min_samples_split': [9]}\n",
    "forest_reg = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(forest_reg,param_grid=param_grid)\n",
    "grid_search.fit(features,target)\n",
    "predictions=grid_search.predict(features_test)\n",
    "print classification_report(target_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'bootstrap': [False], 'min_samples_leaf': [1], 'min_samples_split': [9], 'criterion': ['entropy'], 'max_features': [1], 'max_depth': [None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_\n",
    "grid_search\n",
    "#grid_search.cv_results_['params'][grid_search.best_index_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=20, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'min_samples_leaf': [1, 2], 'n_estimators': [10, 20], 'min_samples_split': [2, 4], 'min_weight_fraction_leaf': [0.0], 'max_features': [None], 'max_depth': [3, 4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code for radient boosting classifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf_gbc = GradientBoostingClassifier(n_estimators=20,max_features=None,\n",
    "                                     min_samples_split=2, min_samples_leaf=1,\n",
    "                                     min_weight_fraction_leaf=0.0, max_depth=3)\n",
    "#grid search for GBC..first ran the grid search for a # of parameters and then found the best estimate and redid the score\n",
    "param_grid = {\n",
    "   'n_estimators':[10,20],\n",
    "    'max_features':[None],\n",
    "    'min_samples_split':[2,4],\n",
    "    'min_samples_leaf':[1,2],\n",
    "    'min_weight_fraction_leaf':[0.0],\n",
    "    'max_depth':[3,4]}\n",
    "# param_grid = {'criterion':'friedman_mse', 'init':[None],\n",
    "#               'learning_rate':[0.1], 'loss':'deviance', 'max_depth':[3],\n",
    "#               'max_features':[None], 'max_leaf_nodes':[None],\n",
    "#               'min_impurity_split':[1e-07], 'min_samples_leaf':[2],\n",
    "#               'min_samples_split':[2], 'min_weight_fraction_leaf':[0.0],\n",
    "#               'n_estimators':[20], 'presort':['auto'], 'random_state':None,\n",
    "#               'subsample':[1.0],'verbose':[0], 'warm_start':[False]\n",
    "#                }\n",
    "grid_search_gbc = GridSearchCV(clf_gbc,param_grid=param_grid)\n",
    "grid_search_gbc.fit(features,target)\n",
    "\n",
    "#target_predict=clf_gbc.predict(features_test)\n",
    "#clf_gbc.score(target_test,target_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=4, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=20, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_gbc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.93      0.96      0.94        26\n",
      "        1.0       0.67      0.50      0.57         4\n",
      "\n",
      "avg / total       0.89      0.90      0.89        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_gbc=grid_search_gbc.predict(features_test)\n",
    "print classification_report(target_test,predictions_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
